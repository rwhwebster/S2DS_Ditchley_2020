{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ditchley S2DS project August 2020 - Pipeline B<h1>\n",
    "    <h2>Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster<h2>\n",
    "        \n",
    "This notebook (B_pipeline) will be dedicated to:\n",
    "- B1: Working directory and keyword setting\n",
    "- B2: Selection of inliers based on distribution of friends and followers (optional)       \n",
    "- B3: Download equal amounts of journalists and friends tweets and clean them\n",
    "- B4: Calculate H-Index for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B1: Working directory and keyword setting<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and set up working directory\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up working directory\n",
    "# The working directory should reflect the structure of the Github repository https://github.com/S2DSLondon/Aug20_Ditchley\n",
    "sys.path.insert(1, 'C:/Users/Luca/Aug20_Ditchley/')\n",
    "from src.data import pipeline_setup\n",
    "pipeline_setup.build_data_dir('C:/Users/Luca/Aug20_Ditchley/')\n",
    "\n",
    "#Set again the keyword of interest\n",
    "keyword = 'cybersecurity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B2: Selection of inliers based on distribution of friends and followers<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the csv that store journalist user data with the one storing friends user data.\n",
    "# We do this to calculate static measures for all users at the same time\n",
    "user_profiles = pd.read_csv('../data/processed/'+keyword+'_user_profiles.csv' )\n",
    "user_friends_profiles = pd.read_csv('../data/processed/'+keyword+'_user_friends_profiles.csv' )\n",
    "users_df = pd.concat([user_profiles,user_friends_profiles])\n",
    "users_df = users_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from src.graph_database import graphdb as gdb\n",
    "\n",
    "# This function calculate the chisquare for each user\n",
    "no_loners = gdb.get_chi2(users_df)\n",
    "\n",
    "#We can then classify each user as an inlier or outlier based on their chisquare value\n",
    "inliers = no_loners[no_loners['chi2']<6.18]\n",
    "outliers = no_loners[no_loners['chi2']>6.18]\n",
    "\n",
    "# This function plots the distribution of inliers/outliers\n",
    "from src import plots\n",
    "inliner_plot = plots.plot_user_inliers(inliers, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and update the user_profiles csv with chisquares values\n",
    "no_loners.to_csv('../data/processed/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B3: Download equal amounts of journalists and friends tweets and clean them<h3>\n",
    "    <h4>B3.1: Tweets download using Twitter API<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this step to dowload an equal number of tweets from all users (journalists and friends) in order to carry out the H-Index on the same amounts of tweets\n",
    "df_friends_handles = pd.read_csv('../data/processed/'+keyword+'_journalist_friends.csv')\n",
    "df_user_list_extended = pd.concat([pd.DataFrame(df_friends_handles.screen_name.unique()),df_friends_handles.friend]).drop_duplicates().reset_index(drop=True)\n",
    "df_user_list_extended.columns = ['user']\n",
    "users = df_user_list_extended.user.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')\n",
    "# Import fuctions\n",
    "from src.data.api_tweet_tools import request_user_timeline, batch_request_user_timeline\n",
    "\n",
    "# Define the destination folder in which the csv files will be saved\n",
    "dest_dir = '../data/raw/friends_tweets/'\n",
    "# If the folder does not exist, this will create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Finally, dowload the tweets for all the jornalists friends. \n",
    "# Input: tw_api = tweepy.api.API, list = list of friends from df, filepath = destination folder set above, \n",
    "# api_delay = seconds of delay between requests |note: if you reach 900 requests in less that 15 min the function will pause and \n",
    "# then resume when the 15 min have passed|, n_tweets = max 200 for a single request\n",
    "batch_request_user_timeline(tw_api, users, filepath = dest_dir, api_delay = 0.2, n_tweets=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.2: Tweet data cleaning<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweet dataframes\n",
    "from src.data import data_cleanup as dc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define source and destination folders for the cleaning function\n",
    "src_dir = '../data/raw/friends_tweets/'\n",
    "dest_dir = '../data/cleaned/friends_tweets/'\n",
    "# If the folder does not exist, this will create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Loop through the files, load 1 csv at the time, clean it, and save it to destination folder\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        raw_df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        cleaned_df = dc.clean_API_dataframe(raw_df)\n",
    "        cleaned_df.to_csv(os.path.join(dest_dir, file), index=False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B4: Calculate H-Index<h3>\n",
    "    <h4>B4.1: H-Index function<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and functions\n",
    "from src.data import H_Index_tools as h_tools\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define source and destination folders for the H_index function\n",
    "src_dir = '../data/cleaned/friends_tweets/'\n",
    "dest_dir = '../data/processed/'\n",
    "# This function will loop throught the users tweet csv files and calculate H-Index and the sum of retweets&likes for each one\n",
    "h_tools.loop_csv_H_index(src_dir,dest_dir, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.2: Merge the results of H-Index with exisiting user profiles<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import user profiles\n",
    "df_users_profiles = pd.read_csv('../data/processed/'+keyword+'_user_profiles.csv')\n",
    "\n",
    "\n",
    "# Merge the dataframe of all users (journalists + friends) with the csv with sums of their likes and retweets and the csv with thier H-Index\n",
    "# Load RT and like count dataframe\n",
    "like_rt_count_users = pd.read_csv('../data/processed/'+keyword+'_like_rt_count_users.csv' )\n",
    "# Load H-index dataframe \n",
    "h_index_users = pd.read_csv('../data/processed/'+keyword+'_h_index_users.csv' )\n",
    "# Merge 1\n",
    "df_user_profiles_metrics = pd.merge(df_users_profiles, like_rt_count_users, how='inner', on='screen_name')\n",
    "# Merge 2\n",
    "df_users_profiles_metrics = pd.merge(df_user_profiles_metrics, h_index_users, how='inner', on='screen_name')\n",
    "# Drop duplicates (the journalists rows sometimes are repeated)\n",
    "df_users_profiles_metrics.drop_duplicates(subset ='screen_name',keep = 'first', inplace = True)\n",
    "# Save final df\n",
    "df_users_profiles_metrics.to_csv('../data/processed/'+keyword+'_users_profiles_metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.3: Plot the results of H-Index<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 plots: \n",
    "# 1. Distribution of H-index values. \n",
    "# 2. Correlation between n of followers and n of friends and H-Index\n",
    "from src import plots\n",
    "plots_H_index = plots.plot_H_index(df_users_profiles_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
