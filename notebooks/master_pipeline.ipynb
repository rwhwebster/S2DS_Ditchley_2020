{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ditchley S2DS project August 2020 - Code Pipeline<h1>\n",
    "    <h2>Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![]() #graphical representation of the pipeline here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 0: Working directory and graph DB setup<h3>\n",
    "    <h4>0.1: Modules and working directory setup<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory & sub-directories already exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Import modules and set up working directory\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up working directory\n",
    "# The working directory should reflect the structure of the Github repository https://github.com/S2DSLondon/Aug20_Ditchley\n",
    "sys.path.insert(1, 'C:/Users/Luca/Aug20_Ditchley/')\n",
    "from src.data import pipeline_setup\n",
    "pipeline_setup.build_data_dir('C:/Users/Luca/Aug20_Ditchley/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>0.2: Initialize graph database<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from src.data import graphdb as gdb\n",
    "\n",
    "# load / declare the database\n",
    "graph = gdb.get_graph(new_graph = True)\n",
    "graph\n",
    "# start with an empty graph\n",
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 1: Getting journalist twitter handles according to a keyword<h3>\n",
    "    <h4>The journalist scraping is performed at the web address https://www.journalism.co.uk/prof/?chunk=0&cmd=default<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jennystrasburg',\n",
       " 'dannsimmons',\n",
       " 'LeoKelion',\n",
       " 'gordoncorera',\n",
       " 'joetidy',\n",
       " '_lucyingham',\n",
       " 'dannyjpalmer',\n",
       " 'SophiaFurber',\n",
       " 'SCFGallagher',\n",
       " 'MsHannahMurphy',\n",
       " 'JesscaHaworth',\n",
       " 'Ad_Nauseum74']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose keyword and run the scraping function\n",
    "from src.data import journalists as journos\n",
    "keyword = 'cyber'\n",
    "# Input: string / Output: list\n",
    "journo_handles = journos.get_handles_by_keyword(keyword)\n",
    "print(len(journo_handles))\n",
    "journo_handles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 2. Scrape user information and friend lists for each journalist in the list<h3>\n",
    "    <h4>2.1: Scrape user information using the Twitter API<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "\n",
    "# Input: tweepy.api.API,list / Output: list\n",
    "api_users = api_tools.batch_request_user_info(tw_api,journo_handles)\n",
    "# Input: list / Output: DataFrame\n",
    "df_api = dc.populate_user_df(api_users)\n",
    "# Check\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as csv\n",
    "df_api.to_csv('../data/processed/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.2: Load user info into graph DB<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j import files need to be in a specific folder, however, the csv files saved above are in a different folder, to go around this problem on Windows machines it is\n",
    "# possible to create a shortcut between the two folders\n",
    "\n",
    "# lowd in user information\n",
    "print('Loading in user information and drawing (Person) nodes')\n",
    "fn_users = 'cybersecurity_user_profiles.csv'\n",
    "gdb.load_users(fn_users ,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.3: Scrape user friend list using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from src.data import twint_tools as tt\n",
    "\n",
    "# define keyword arguments / 'n_retries' = max number of scrape attempts, 'suppress' = hide critical Twint warnings\n",
    "kwargs = {'n_retries':5,\n",
    "         'suppress':False}\n",
    "# Multi threading function Input: _get_friends function, number of threads to distribute the queque, args and kwargs\n",
    "tt.twint_in_queue(tt._get_friends, 6, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the individual lists into one dataframe with journalist and its friends\n",
    "friends_csv = tt.join_friends_csv(journo_handles,keyword) # this function has a bug, the first friend name is 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as csv\n",
    "friends_csv.to_csv('../data/processed/'+keyword+'_journalist_friends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.4: Load friend information into DB<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in friend information\n",
    "print('Loading in friends info and drawing [FOLLOWS] edges')\n",
    "fn_friends = 'cybersecurity_journalist_friends.csv'\n",
    "gdb.load_friends(fn_friends,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 3. Loop over selected journalists handles and scrape their tweets (3.1) and mentions (3.2) using Twint<h3>\n",
    "    <h4>Section 3.1: Scrape tweets using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {'date_range':('2020-08-01 00:00:00', None),\n",
    "         'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._search_tweets_by_user, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined all the individual csv into one dataframe\n",
    "cyber_test = tt.join_tweet_csv(journo_handles, keyword)\n",
    "# Check\n",
    "cyber_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as csv\n",
    "cyber_test.to_csv('../data/processed/'+keyword+'_journalist_tweets_twint.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.2: Extract mentions from Twint dataset<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_cleanup as dc\n",
    "# from the twint dataset, extract mentions based on tweet id and save in a separate csv\n",
    "mentions_twint  = dc.mentions_to_df(cyber_test)\n",
    "# Check\n",
    "mentions_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "mentions_twint.to_csv('../data/processed/' + keyword + '_mentions_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 4. Loop over selected journalists handles and scrape their tweets (4.1) and mentions (4.2) using Twitter API<h3>\n",
    "    <h4>Section 4.1: Scrape tweets using Twitter API<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.api_tweet_tools import request_user_timeline, batch_request_user_timeline\n",
    "cyber_test_api = batch_request_user_timeline(tw_api, journo_handles, '../data/processed/',  n_tweets=3200)\n",
    "\n",
    "# Check\n",
    "cyber_test_api.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 4.2: Extract mentions from API tweets<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_cleanup as dc\n",
    "# from the API dataset, extract mentions based on tweet id and save in a separate csv\n",
    "mentions_api  = dc.mentions_to_df(cyber_test_api)\n",
    "# Check\n",
    "mentions_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_api.to_csv('../data/processed/' + keyword + '_mentions_api.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 5. Data cleaning and standardization/LDA<h3>\n",
    "     <h4>Section 5.1: Clean and standardize Twint dataset<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the twint output \n",
    "from src.data import data_cleanup as dc\n",
    "\n",
    "# Standardize Twint dataset for graph DB loading\n",
    "standard_tweet_twint = dc.clean_twint_dataframe(cyber_test)\n",
    "# Check\n",
    "standard_tweet_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "standard_tweet_twint.to_csv('../data/processed/' + keyword + '_standard_tweets_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 5.2: Clean and standardize API dataset<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the twint output \n",
    "from src.data import data_cleanup as dc\n",
    "\n",
    "# Standardize API dataset for graph DB loading\n",
    "standard_tweet_api = dc.clean_API_dataframe(cyber_test_api)\n",
    "# Check\n",
    "standard_tweet_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "standard_tweet_api.to_csv('../data/processed/' + keyword + '_standard_tweets_api.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 6. Create graph database and import twitter data into it<h3>\n",
    "    <h4>Section 6.1: Import modules and load graph database<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from src.data import graphdb as gdb\n",
    "\n",
    "# load / declare the database\n",
    "graph = gdb.get_graph(new_graph = True)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.2: Load user info into graph DB<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in user information and drawing (Person) nodes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-97b38e02d53e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading in user information and drawing (Person) nodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfn_users\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cybersecurity_user_profiles.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn_users\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gdb' is not defined"
     ]
    }
   ],
   "source": [
    "# Neo4j import files need to be in a specific folder, however, the csv files saved above are in a different folder, to go around this problem on Windows machines it is\n",
    "# possible to create a shortcut between the two folders\n",
    "\n",
    "# lowd in user information\n",
    "print('Loading in user information and drawing (Person) nodes')\n",
    "fn_users = 'cybersecurity_user_profiles.csv'\n",
    "gdb.load_users(fn_users ,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.2: Load friend information into DB<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in friend information\n",
    "print('Loading in friends info and drawing [FOLLOWS] edges')\n",
    "fn_friends = 'cybersecurity_journalist_friends.csv'\n",
    "gdb.load_friends(fn_friends,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.3: Load tweet data into DB<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweet information from twint\n",
    "print('Loading in tweets and drawing (Tweet) nodes')\n",
    "fn_tweets = '/data/processed/cybersecurity_standard_tweets_twint.csv'\n",
    "gdb.load_tweets(fn_tweets ,graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweet information from API\n",
    "print('Loading in tweets and drawing (Tweet) nodes')\n",
    "fn_tweets = '/data/processed/cybersecurity_standard_tweets_api.csv'\n",
    "gdb.load_tweets(fn_tweets ,graph) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.4: Draw edges between users and their tweets<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw edges between users and their tweets\n",
    "print('Drawing [POSTS] edges')\n",
    "gdb.get_posts(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.5: Load tweets' mentions<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mentions information\n",
    "print('Loading in mentions and drawing [MENTIONS] edges')\n",
    "fn_mentions = 'cybersecurity_mentions_twint.csv'\n",
    "gdb.load_mentions(fn_mentions,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.6: Run page rank algorithm using [FOLLOWS] [MENTIONS] edges<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Page rank using follower and mention edges\n",
    "print('running page rank')\n",
    "nodelist = ['Person','Tweet']\n",
    "edgelist = ['FOLLOWS','MENTIONS']\n",
    "page_rank_friends_mentions = gdb.run_pagerank(nodelist,edgelist,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.7: Get a weighted random sample from the journalists friends<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a weighted random sample of users\n",
    "n_sample = 20\n",
    "fields = ['rank']\n",
    "exponents = [2]\n",
    "sample = gdb.get_multiple_weighted_sample(page_rank_friends_mentions,n_sample,fields,exponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 7. Download friends user info<h3>\n",
    "    <h4>7.1. Join journalists list with friends list ######## maybe this step is not necessary? What if we join dataframes of friends user lists and initial journalists?<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this step to dowload an equal number of tweets from all users in order to carry out the topic analysis on the same tweet sample\n",
    "df_friends_handles = pd.read_csv('../data/processed/'+keyword+'_journalist_friends.csv')\n",
    "df_user_list_extended = pd.concat([pd.DataFrame(df_friends_handles.screen_name.unique()),df_friends_handles.friend]).drop_duplicates().reset_index(drop=True)\n",
    "df_user_list_extended.columns = ['user']\n",
    "freinds = list(df_user_list_extended.user)\n",
    "friends = df_user_list_extended.user.tolist()\n",
    "#friends.remove('hoarsewisperer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends[140]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>7.2. Download the user information for the new extended list of users<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')\n",
    "\n",
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "#from src.data.api_tweet_tools import request_user_timeline, batch_request_user_timeline\n",
    "# Input: tweepy.api.API,list / Output: list\n",
    "api_users_friends = api_tools.batch_request_user_info(tw_api, friends)\n",
    "# Input: list / Output: DataFrame\n",
    "df_api_user_friends = dc.populate_user_df(api_users_friends)\n",
    "# Save the dataframe as csv\n",
    "df_api_user_friends.to_csv('../data/processed/'+keyword+'_user_friends_profiles.csv', index = False)\n",
    "# Check\n",
    "df_api_user_friends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 8. Subsample profiles based on follower/friends distribution<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_friends_n</th>\n",
       "      <th>user_followers_n</th>\n",
       "      <th>prof_created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>h-index_like&amp;retweets</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>244169661</td>\n",
       "      <td>jennystrasburg</td>\n",
       "      <td>jenny strasburg</td>\n",
       "      <td>London</td>\n",
       "      <td>wsj reporter new mexican in london cyber crime...</td>\n",
       "      <td>2852</td>\n",
       "      <td>5583</td>\n",
       "      <td>2011-01-28 17:33:33</td>\n",
       "      <td>5002</td>\n",
       "      <td>True</td>\n",
       "      <td>8444</td>\n",
       "      <td>216</td>\n",
       "      <td>344</td>\n",
       "      <td>9</td>\n",
       "      <td>0.757568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21644992</td>\n",
       "      <td>dannsimmons</td>\n",
       "      <td>Dan Simmons</td>\n",
       "      <td>London, England</td>\n",
       "      <td>bbc technology reporter on specialises in mobi...</td>\n",
       "      <td>295</td>\n",
       "      <td>12457</td>\n",
       "      <td>2009-02-23 10:53:15</td>\n",
       "      <td>834</td>\n",
       "      <td>True</td>\n",
       "      <td>2247</td>\n",
       "      <td>215</td>\n",
       "      <td>1019</td>\n",
       "      <td>8</td>\n",
       "      <td>0.590327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19883587</td>\n",
       "      <td>leokelion</td>\n",
       "      <td>Leo Kelion</td>\n",
       "      <td>London</td>\n",
       "      <td>technology desk editor of bbc news recently wo...</td>\n",
       "      <td>4774</td>\n",
       "      <td>11865</td>\n",
       "      <td>2009-02-01 23:40:21</td>\n",
       "      <td>3482</td>\n",
       "      <td>True</td>\n",
       "      <td>4161</td>\n",
       "      <td>1057</td>\n",
       "      <td>2415</td>\n",
       "      <td>9</td>\n",
       "      <td>1.350548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>242355547</td>\n",
       "      <td>gordoncorera</td>\n",
       "      <td>Gordon Corera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbc security correspondent author russians amo...</td>\n",
       "      <td>1293</td>\n",
       "      <td>13706</td>\n",
       "      <td>2011-01-24 15:59:31</td>\n",
       "      <td>244</td>\n",
       "      <td>False</td>\n",
       "      <td>1792</td>\n",
       "      <td>4007</td>\n",
       "      <td>7260</td>\n",
       "      <td>38</td>\n",
       "      <td>0.139076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16527091</td>\n",
       "      <td>joetidy</td>\n",
       "      <td>Joe Tidy</td>\n",
       "      <td>DMs Open</td>\n",
       "      <td>cyber reporter for bbc news covering cyber sec...</td>\n",
       "      <td>2722</td>\n",
       "      <td>11002</td>\n",
       "      <td>2008-09-30 12:51:28</td>\n",
       "      <td>8447</td>\n",
       "      <td>True</td>\n",
       "      <td>13518</td>\n",
       "      <td>341</td>\n",
       "      <td>1134</td>\n",
       "      <td>16</td>\n",
       "      <td>0.629276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18543</th>\n",
       "      <td>798873782211448833</td>\n",
       "      <td>notameadow</td>\n",
       "      <td>🌻 𝙼𝚎𝚊𝚍𝚘𝚠 0x1338 𝙴𝚕𝚕𝚒𝚜 🌻</td>\n",
       "      <td>London, England</td>\n",
       "      <td>foil sealed for freshness of what i say here i...</td>\n",
       "      <td>1676</td>\n",
       "      <td>14213</td>\n",
       "      <td>2016-11-16 13:02:06</td>\n",
       "      <td>58596</td>\n",
       "      <td>False</td>\n",
       "      <td>44598</td>\n",
       "      <td>31</td>\n",
       "      <td>569</td>\n",
       "      <td>12</td>\n",
       "      <td>0.262938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18544</th>\n",
       "      <td>519989509</td>\n",
       "      <td>mozillasecurity</td>\n",
       "      <td>Mozilla Security</td>\n",
       "      <td>NaN</td>\n",
       "      <td>official account of mozilla security</td>\n",
       "      <td>55</td>\n",
       "      <td>686</td>\n",
       "      <td>2012-03-10 01:36:23</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>348</td>\n",
       "      <td>472</td>\n",
       "      <td>10</td>\n",
       "      <td>3.488539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18545</th>\n",
       "      <td>804230797993443328</td>\n",
       "      <td>sudo_sudoka</td>\n",
       "      <td>Sudoka</td>\n",
       "      <td>The Matrix (Inside now)</td>\n",
       "      <td>threat analyst bounty hunter ctf player securi...</td>\n",
       "      <td>151</td>\n",
       "      <td>385</td>\n",
       "      <td>2016-12-01 07:48:58</td>\n",
       "      <td>967</td>\n",
       "      <td>False</td>\n",
       "      <td>268</td>\n",
       "      <td>234</td>\n",
       "      <td>732</td>\n",
       "      <td>7</td>\n",
       "      <td>2.215415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18546</th>\n",
       "      <td>836272957</td>\n",
       "      <td>nordvpn</td>\n",
       "      <td>NordVPN</td>\n",
       "      <td>Privacy solutions</td>\n",
       "      <td>everyone deserves a secure private and unrestr...</td>\n",
       "      <td>3515</td>\n",
       "      <td>77283</td>\n",
       "      <td>2012-09-20 20:07:18</td>\n",
       "      <td>2634</td>\n",
       "      <td>True</td>\n",
       "      <td>19989</td>\n",
       "      <td>238</td>\n",
       "      <td>1195</td>\n",
       "      <td>25</td>\n",
       "      <td>1.576031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>543802336</td>\n",
       "      <td>torguard</td>\n",
       "      <td>TorGuard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anonymous vpn proxy and email services privacy...</td>\n",
       "      <td>2</td>\n",
       "      <td>9459</td>\n",
       "      <td>2012-04-02 23:07:17</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>1427</td>\n",
       "      <td>71</td>\n",
       "      <td>341</td>\n",
       "      <td>11</td>\n",
       "      <td>16.491761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18344 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id      screen_name                     name  \\\n",
       "0               244169661   jennystrasburg          jenny strasburg   \n",
       "2                21644992      dannsimmons              Dan Simmons   \n",
       "4                19883587        leokelion               Leo Kelion   \n",
       "10              242355547     gordoncorera            Gordon Corera   \n",
       "12               16527091          joetidy                 Joe Tidy   \n",
       "...                   ...              ...                      ...   \n",
       "18543  798873782211448833       notameadow  🌻 𝙼𝚎𝚊𝚍𝚘𝚠 0x1338 𝙴𝚕𝚕𝚒𝚜 🌻   \n",
       "18544           519989509  mozillasecurity         Mozilla Security   \n",
       "18545  804230797993443328      sudo_sudoka                   Sudoka   \n",
       "18546           836272957          nordvpn                  NordVPN   \n",
       "18547           543802336         torguard                 TorGuard   \n",
       "\n",
       "                      location  \\\n",
       "0                       London   \n",
       "2              London, England   \n",
       "4                       London   \n",
       "10                         NaN   \n",
       "12                    DMs Open   \n",
       "...                        ...   \n",
       "18543          London, England   \n",
       "18544                      NaN   \n",
       "18545  The Matrix (Inside now)   \n",
       "18546        Privacy solutions   \n",
       "18547                      NaN   \n",
       "\n",
       "                                        user_description  user_friends_n  \\\n",
       "0      wsj reporter new mexican in london cyber crime...            2852   \n",
       "2      bbc technology reporter on specialises in mobi...             295   \n",
       "4      technology desk editor of bbc news recently wo...            4774   \n",
       "10     bbc security correspondent author russians amo...            1293   \n",
       "12     cyber reporter for bbc news covering cyber sec...            2722   \n",
       "...                                                  ...             ...   \n",
       "18543  foil sealed for freshness of what i say here i...            1676   \n",
       "18544               official account of mozilla security              55   \n",
       "18545  threat analyst bounty hunter ctf player securi...             151   \n",
       "18546  everyone deserves a secure private and unrestr...            3515   \n",
       "18547  anonymous vpn proxy and email services privacy...               2   \n",
       "\n",
       "       user_followers_n      prof_created_at  favourites_count  verified  \\\n",
       "0                  5583  2011-01-28 17:33:33              5002      True   \n",
       "2                 12457  2009-02-23 10:53:15               834      True   \n",
       "4                 11865  2009-02-01 23:40:21              3482      True   \n",
       "10                13706  2011-01-24 15:59:31               244     False   \n",
       "12                11002  2008-09-30 12:51:28              8447      True   \n",
       "...                 ...                  ...               ...       ...   \n",
       "18543             14213  2016-11-16 13:02:06             58596     False   \n",
       "18544               686  2012-03-10 01:36:23                23     False   \n",
       "18545               385  2016-12-01 07:48:58               967     False   \n",
       "18546             77283  2012-09-20 20:07:18              2634      True   \n",
       "18547              9459  2012-04-02 23:07:17               178     False   \n",
       "\n",
       "       statuses_count  retweet_count  like_count  h-index_like&retweets  \\\n",
       "0                8444            216         344                      9   \n",
       "2                2247            215        1019                      8   \n",
       "4                4161           1057        2415                      9   \n",
       "10               1792           4007        7260                     38   \n",
       "12              13518            341        1134                     16   \n",
       "...               ...            ...         ...                    ...   \n",
       "18543           44598             31         569                     12   \n",
       "18544              68            348         472                     10   \n",
       "18545             268            234         732                      7   \n",
       "18546           19989            238        1195                     25   \n",
       "18547            1427             71         341                     11   \n",
       "\n",
       "            chi2  \n",
       "0       0.757568  \n",
       "2       0.590327  \n",
       "4       1.350548  \n",
       "10      0.139076  \n",
       "12      0.629276  \n",
       "...          ...  \n",
       "18543   0.262938  \n",
       "18544   3.488539  \n",
       "18545   2.215415  \n",
       "18546   1.576031  \n",
       "18547  16.491761  \n",
       "\n",
       "[18344 rows x 15 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.graph import graphdb as g_db\n",
    "g_db.get_chi2(df_users_profiles_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 9. Download 200 tweets for each user<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_user_list_extended.user[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')\n",
    "\n",
    "from src.data.api_tweet_tools import request_user_timeline, batch_request_user_timeline\n",
    "#batch_request_user_timeline(tw_api, list(df_user_list_extended.user[:100]), filepath = '../data/processed/', api_delay = .3, n_tweets=2)\n",
    "batch_request_user_timeline(tw_api, friends, filepath = '../data/raw/users_tweets/', api_delay = 0.1, n_tweets=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 10. Topic modelling and H-index calculation<h3>\n",
    "    <h4>10.1. Tweet cleaning and standardization<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "from src.data import data_cleanup as dc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "src_dir = '../data/raw/users_tweets/'\n",
    "dest_dir = '../data/cleaned/users_tweets/'\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        raw_df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        cleaned_df = dc.clean_API_dataframe(raw_df)\n",
    "        cleaned_df.to_csv(os.path.join(dest_dir, file), index=False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>10.2. Calculate H-Index for each user (excluding RT)<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import H_Index_tools as h_tools\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "src_dir = '../data/cleaned/users_tweets/'\n",
    "dest_dir = '../data/processed/'\n",
    "h_tools.loop_csv_H_index(src_dir,dest_dir, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import journalist user profiles, friends user profiles and concatenate them\n",
    "user_profiles = pd.read_csv('../data/processed/'+keyword+'_user_profiles.csv' )\n",
    "user_friends_profiles = pd.read_csv('../data/processed/'+keyword+'_user_friends_profiles.csv' )\n",
    "users_df = pd.concat([user_profiles,user_friends_profiles])\n",
    "\n",
    "# Join the dataframe of all users (journalists + friends) with the sums of their likes, retweets and thier H-Index\n",
    " # Import\n",
    "like_rt_count_users = pd.read_csv('../data/processed/'+keyword+'_like_rt_count_users.csv' )\n",
    "h_index_users = pd.read_csv('../data/processed/'+keyword+'_h_index_users.csv' )\n",
    "# Merge 1\n",
    "df_user_profiles_metrics = pd.merge(users_df, like_rt_count_users, how='inner', on='screen_name')\n",
    "# Merge 2\n",
    "df_users_profiles_metrics = pd.merge(df_user_profiles_metrics, h_index_users, how='inner', on='screen_name')\n",
    "# Drop duplicates (the journalists rows sometimes are repeated)\n",
    "df_users_profiles_metrics.drop_duplicates(subset ='screen_name',keep = 'first', inplace = True)\n",
    "# Save final df\n",
    "df_users_profiles_metrics.to_csv('../data/processed/'+keyword+'_user_profiles_metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>10.3. Topic modelling<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
